from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.core.llm_factory import get_llm  # å¤ç”¨ä½ ä¹‹å‰çš„ LLM å·¥å‚

# è·¯å¾„é…ç½®
DB_LOAD_PATH = "blog_faiss_index"


def query_blog_knowledge(question: str):
    # 1. åŠ è½½æ¨¡å‹å’Œå‘é‡åº“
    embedding_model = HuggingFaceBgeEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    try:
        vector_store = FAISS.load_local(
            DB_LOAD_PATH,
            embedding_model,
            allow_dangerous_deserialization=True
        )
    except Exception:
        return "âŒ æ‰¾ä¸åˆ°çŸ¥è¯†åº“ï¼Œè¯·å…ˆè¿è¡Œ build_blog_kb.py"

    # 2. æ£€ç´¢ (Retrieve)
    # k=3 è¡¨ç¤ºæ‰¾æœ€ç›¸å…³çš„3ä¸ªç‰‡æ®µ
    docs = vector_store.similarity_search(question, k=3)

    if not docs:
        return "åšå®¢é‡Œå¥½åƒæ²¡æœ‰ç›¸å…³å†…å®¹ã€‚"

    # æ‹¼æ¥ä¸Šä¸‹æ–‡
    context = "\n\n".join([f"---ç‰‡æ®µæ¥æº: {d.metadata['source']}---\n{d.page_content}" for d in docs])

    # 3. ç”Ÿæˆ (Generate)
    llm = get_llm(temperature=0.3)

    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä¸ªåŸºäºä¸ªäººåšå®¢çš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„åšå®¢å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        å¦‚æœåšå®¢å†…å®¹é‡Œæ²¡æœ‰æåˆ°ï¼Œè¯·ç›´æ¥è¯´â€œåšå®¢é‡Œæ²¡æœ‰æ¶‰åŠè¯¥è¯é¢˜â€ã€‚

        ã€åšå®¢å†…å®¹ç‰‡æ®µã€‘ï¼š
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
        {question}
        """
    )

    chain = prompt | llm | StrOutputParser()

    print(f"ğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡ç« : {[d.metadata['source'] for d in docs]}")

    # æµå¼è¾“å‡ºæˆ–ç›´æ¥è¾“å‡º
    response = chain.invoke({"context": context, "question": question})
    return response


if __name__ == "__main__":
    q = input("è¯·è¾“å…¥ä½ æƒ³æŸ¥è¯¢åšå®¢çš„é—®é¢˜: ")
    answer = query_blog_knowledge(q)
    print("\nğŸ¤– AI å›ç­”:\n", answer)