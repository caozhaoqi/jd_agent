import os
import sys
from dotenv import load_dotenv

# ==========================================
# ğŸ”´ æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶åŠ è½½é¡¹ç›®æ ¹ç›®å½•çš„ .env æ–‡ä»¶
# ==========================================
# è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
current_path = os.path.abspath(__file__)
# å‘å›é€€ 4 å±‚æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½• (æ ¹æ®ä½ çš„ç›®å½•ç»“æ„: src/app/blog/query_blog.py)
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_path))))
env_path = os.path.join(project_root, ".env")

# 1. åŠ è½½ç¯å¢ƒå˜é‡
if os.path.exists(env_path):
    load_dotenv(env_path)
    print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡: {env_path}")
else:
    print(f"âŒ è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œè·¯å¾„: {env_path}")

# 2. å°† src ç›®å½•åŠ å…¥ Python æœç´¢è·¯å¾„ï¼Œé˜²æ­¢ 'ModuleNotFoundError: No module named app'
src_path = os.path.join(project_root, "src")
if src_path not in sys.path:
    sys.path.append(src_path)
# ==========================================

# ğŸ”´ ä¿®å¤ä¾èµ–å¯¼å…¥
# å¿…é¡»å…ˆå®‰è£…æ–°ç‰ˆåº“: pip install langchain-huggingface
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from app.core.llm_factory import get_llm

# è·¯å¾„é…ç½® (æŒ‡å‘ç”Ÿæˆçš„å‘é‡åº“æ–‡ä»¶å¤¹)
DB_LOAD_PATH = "../../../blog_faiss_index"


def query_blog_knowledge(question: str):
    # 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (ä½¿ç”¨æ–°ç‰ˆ)
    print("â³ æ­£åœ¨åŠ è½½ BGE æ¨¡å‹...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    try:
        # åŠ è½½å‘é‡åº“
        vector_store = FAISS.load_local(
            DB_LOAD_PATH,
            embedding_model,
            allow_dangerous_deserialization=True
        )
    except Exception as e:
        return f"âŒ æ‰¾ä¸åˆ°çŸ¥è¯†åº“ç›®å½• '{DB_LOAD_PATH}'ã€‚\nè¯·å…ˆç¡®ä¿ä½ è¿è¡Œäº† build_blog_kb.py å¹¶ä¸”ç”Ÿæˆäº†ç´¢å¼•æ–‡ä»¶ã€‚\né”™è¯¯è¯¦æƒ…: {e}"

    # 2. æ£€ç´¢ (Retrieve)
    print(f"ğŸ” æ­£åœ¨æ£€ç´¢é—®é¢˜: {question}")
    docs = vector_store.similarity_search(question, k=3)

    if not docs:
        return "åšå®¢é‡Œå¥½åƒæ²¡æœ‰ç›¸å…³å†…å®¹ã€‚"

    # æ‹¼æ¥ä¸Šä¸‹æ–‡
    context = "\n\n".join([f"---ç‰‡æ®µæ¥æº: {d.metadata.get('source', 'æœªçŸ¥')}---\n{d.page_content}" for d in docs])

    # 3. ç”Ÿæˆ (Generate)
    llm = get_llm(temperature=0.3)

    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä¸ªåŸºäºä¸ªäººåšå®¢çš„ AI åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„åšå®¢å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
        å¦‚æœåšå®¢å†…å®¹é‡Œæ²¡æœ‰æåˆ°ï¼Œè¯·ç›´æ¥è¯´â€œåšå®¢é‡Œæ²¡æœ‰æ¶‰åŠè¯¥è¯é¢˜â€ã€‚

        ã€åšå®¢å†…å®¹ç‰‡æ®µã€‘ï¼š
        {context}

        ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
        {question}
        """
    )

    chain = prompt | llm | StrOutputParser()

    print(f"ğŸ“„ å‚è€ƒæ–‡ç« : {[d.metadata.get('source') for d in docs]}")

    response = chain.invoke({"context": context, "question": question})
    return response


if __name__ == "__main__":
    # äº¤äº’å¼æŸ¥è¯¢
    while True:
        print("\n" + "=" * 30)
        q = input("è¯·è¾“å…¥ä½ æƒ³æŸ¥è¯¢åšå®¢çš„é—®é¢˜ (è¾“å…¥ q é€€å‡º): ")
        if q.lower() in ['q', 'quit', 'exit']:
            break

        answer = query_blog_knowledge(q)
        print("\nğŸ¤– AI å›ç­”:\n", answer)