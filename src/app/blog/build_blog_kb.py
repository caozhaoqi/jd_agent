import os
import sys

# ğŸ”´ æ ¸å¿ƒä¿®å¤ 1ï¼šè®¾ç½®å›½å†…é•œåƒåŠ é€Ÿ (å¿…é¡»æ”¾åœ¨æœ€å‰é¢ï¼)
# è¿™ä¼šè®©ä¸‹è½½é€Ÿåº¦ä» 0kb/s å˜æˆ 10MB/s
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

import glob
from loguru import logger  # ä½¿ç”¨æˆ‘ä»¬ç»Ÿä¸€çš„æ—¥å¿—åº“
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

# ğŸ”´ æ ¸å¿ƒä¿®å¤ 2ï¼šä½¿ç”¨æ–°ç‰ˆåº“ï¼Œæ¶ˆé™¤ DeprecationWarning
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from tqdm import tqdm

# === é…ç½®åŒºåŸŸ ===
# è¯·ç¡®è®¤ä½ çš„åšå®¢è·¯å¾„æ˜¯å¦æ­£ç¡®
BLOG_DIR = "/Users/caozhaoqi/Downloads/hexo-bamboo-blog/source/_posts"
DB_SAVE_PATH = "blog_faiss_index"

# é…ç½®æ—¥å¿—æ ¼å¼
logger.remove()
logger.add(sys.stderr, format="<green>{time:HH:mm:ss}</green> | <level>{message}</level>")


def init_embedding_model():
    logger.info("â³ æ­£åœ¨é€šè¿‡å›½å†…é•œåƒåŠ è½½ BGE æ¨¡å‹...")
    # ä½¿ç”¨æ–°ç‰ˆ HuggingFaceEmbeddings
    return HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cpu'},  # å¦‚æœä½ æ˜¯ MèŠ¯ç‰‡ Macï¼Œä¹Ÿå¯ä»¥å°è¯• 'mps'
        encode_kwargs={'normalize_embeddings': True}
    )


def load_and_split_markdown(directory: str):
    md_files = glob.glob(os.path.join(directory, "**/*.md"), recursive=True)
    logger.info(f"ğŸ“‚ å‘ç° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")

    all_splits = []

    # 1. æ ‡é¢˜åˆ‡åˆ†è§„åˆ™
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

    # 2. å­—ç¬¦é•¿åº¦åˆ‡åˆ†è§„åˆ™
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )

    for file_path in tqdm(md_files, desc="å¤„ç†è¿›åº¦"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()

            # ç¬¬ä¸€åˆ€ï¼šæŒ‰ Markdown æ ‡é¢˜åˆ‡
            md_header_splits = markdown_splitter.split_text(text)

            # æ³¨å…¥å…ƒæ•°æ®
            for doc in md_header_splits:
                doc.metadata["source"] = os.path.basename(file_path)

            # ç¬¬äºŒåˆ€ï¼šæŒ‰é•¿åº¦åˆ‡
            splits = text_splitter.split_documents(md_header_splits)
            all_splits.extend(splits)

        except Exception as e:
            logger.error(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

    return all_splits


def build_index():
    # 1. åˆå§‹åŒ–æ¨¡å‹
    embedding_model = init_embedding_model()

    # 2. åŠ è½½ä¸åˆ‡åˆ†
    logger.info("ğŸ”ª å¼€å§‹åˆ‡åˆ†æ–‡æ¡£...")
    docs = load_and_split_markdown(BLOG_DIR)

    if not docs:
        logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œè¯·æ£€æŸ¥ BLOG_DIR è·¯å¾„æ˜¯å¦æ­£ç¡®ï¼")
        return

    logger.success(f"âœ… å…±ç”Ÿæˆ {len(docs)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

    # 3. å‘é‡åŒ–å¹¶å»ºåº“
    logger.info("ğŸ§  æ­£åœ¨å‘é‡åŒ– (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    vector_store = FAISS.from_documents(docs, embedding_model)

    # 4. ä¿å­˜
    vector_store.save_local(DB_SAVE_PATH)
    logger.success(f"ğŸ‰ çŸ¥è¯†åº“å·²æ„å»ºå®Œæˆï¼Œä¿å­˜åœ¨: {DB_SAVE_PATH}")


if __name__ == "__main__":
    build_index()