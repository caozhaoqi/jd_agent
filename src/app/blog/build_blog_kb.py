import os
import glob
from typing import List
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain_community.vectorstores import FAISS
from tqdm import tqdm

# === é…ç½®åŒºåŸŸ ===
BLOG_DIR = "/Users/caozhaoqi/Downloads/hexo-bamboo-blog/source/_posts"  # ä½ çš„åšå®¢ Markdown æ–‡ä»¶å¤¹è·¯å¾„
DB_SAVE_PATH = "blog_faiss_index"  # å‘é‡åº“å­˜æ”¾è·¯å¾„

# 1. åˆå§‹åŒ– Embedding æ¨¡å‹ (JDè¦æ±‚: BGE)
print("â³ æ­£åœ¨åŠ è½½ BGE æ¨¡å‹...")
embedding_model = HuggingFaceBgeEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)


def load_and_split_markdown(directory: str):
    """
    åŠ è½½å¹¶åˆ‡åˆ† Markdown æ–‡ä»¶
    ç­–ç•¥ï¼šå…ˆæŒ‰æ ‡é¢˜åˆ‡åˆ†ï¼Œå†æŒ‰å­—ç¬¦é•¿åº¦é€’å½’åˆ‡åˆ†
    """
    md_files = glob.glob(os.path.join(directory, "**/*.md"), recursive=True)
    print(f"ğŸ“‚ å‘ç° {len(md_files)} ä¸ª Markdown æ–‡ä»¶")

    all_splits = []

    # å®šä¹‰ Markdown æ ‡é¢˜åˆ‡åˆ†è§„åˆ™ (ä¿ç•™ç« èŠ‚ç»“æ„)
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

    # å®šä¹‰å­—ç¬¦é•¿åº¦åˆ‡åˆ†è§„åˆ™ (é˜²æ­¢åˆ‡åˆ†åä¾ç„¶è¿‡é•¿)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50
    )

    for file_path in tqdm(md_files, desc="å¤„ç†æ–‡ä»¶ä¸­"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                text = f.read()

            # ç¬¬ä¸€åˆ€ï¼šæŒ‰ Markdown æ ‡é¢˜åˆ‡
            md_header_splits = markdown_splitter.split_text(text)

            # æŠŠæ–‡ä»¶åä½œä¸º source å­˜å…¥ metadataï¼Œæ–¹ä¾¿æº¯æº
            for doc in md_header_splits:
                doc.metadata["source"] = os.path.basename(file_path)

            # ç¬¬äºŒåˆ€ï¼šæŒ‰é•¿åº¦åˆ‡
            splits = text_splitter.split_documents(md_header_splits)
            all_splits.extend(splits)

        except Exception as e:
            print(f"âŒ è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")

    return all_splits


def build_index():
    # 1. åŠ è½½ä¸åˆ‡åˆ†
    docs = load_and_split_markdown(BLOG_DIR)
    print(f"âœ… å…±ç”Ÿæˆ {len(docs)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

    # 2. å‘é‡åŒ–å¹¶å»ºåº“
    print("â³ æ­£åœ¨å‘é‡åŒ– (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)...")
    vector_store = FAISS.from_documents(docs, embedding_model)

    # 3. ä¿å­˜
    vector_store.save_local(DB_SAVE_PATH)
    print(f"ğŸ‰ çŸ¥è¯†åº“å·²æ„å»ºå®Œæˆï¼Œä¿å­˜åœ¨: {DB_SAVE_PATH}")


if __name__ == "__main__":
    build_index()