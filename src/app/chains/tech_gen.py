from typing import List
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from app.core.llm_factory import get_llm
from app.schemas.interview import InterviewQuestion


# è¾…åŠ©æ¨¡å‹ï¼šç”¨äºè§£æåˆ—è¡¨
class QuestionList(BaseModel):
    questions: List[InterviewQuestion]


async def generate_tech_async(
        tech_stack: List[str],
        level: str,
        kb_context: str = ""  # æ–°å¢å‚æ•°
) -> List[InterviewQuestion]:
    llm = get_llm(temperature=0.7)
    parser = PydanticOutputParser(pydantic_object=QuestionList)

    # ğŸ”´ åŠ¨æ€æ„å»º Prompt
    context_instruction = ""
    if kb_context:
        context_instruction = f"""
        ã€å‚è€ƒçŸ¥è¯†åº“ã€‘ï¼š
        ä»¥ä¸‹æ˜¯è¯¥ç”¨æˆ·ä¸ªäººåšå®¢ä¸­çš„ç›¸å…³æŠ€æœ¯ç¬”è®°ï¼Œè¯·ä¼˜å…ˆå‚è€ƒè¿™äº›å†…å®¹æ¥å‡ºé¢˜ï¼Œ
        å¹¶åœ¨â€œå‚è€ƒå›ç­”è¦ç‚¹â€ä¸­æ˜ç¡®æŒ‡å‡ºâ€œå‚è€ƒäº†åšå®¢ä¸­çš„xxxæ¦‚å¿µâ€ã€‚

        {kb_context}
        """

    prompt = ChatPromptTemplate.from_template(
        """
        åŸºäºä»¥ä¸‹æŠ€æœ¯æ ˆ: {tech_stack}
        é’ˆå¯¹ {level} çº§åˆ«çš„å€™é€‰äººï¼Œç”Ÿæˆ 3 é“å…·æœ‰æŒ‘æˆ˜æ€§çš„æŠ€æœ¯é¢è¯•é¢˜ã€‚

        {context_instruction}

        è¦æ±‚ï¼š
        1. é¢˜ç›®è¦æœ‰æ·±åº¦ï¼Œè€ƒå¯Ÿåº•å±‚åŸç†æˆ–å®æˆ˜æ’é”™ã€‚
        2. æ¯é“é¢˜éƒ½è¦æä¾›ç®€ç»ƒçš„å‚è€ƒå›ç­”è¦ç‚¹ã€‚
        3. ç±»åˆ«æ ‡è®°ä¸º 'Technical'ã€‚

        è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡º:
        {format_instructions}
        """
    )

    chain = prompt | llm | parser

    result = await chain.ainvoke({
        "tech_stack": ", ".join(tech_stack),
        "level": level,
        "context_instruction": context_instruction,  # æ³¨å…¥ Prompt
        "format_instructions": parser.get_format_instructions()
    })

    return result.questions