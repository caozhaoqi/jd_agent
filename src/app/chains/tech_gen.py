from typing import List, Optional
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field
from app.core.llm_factory import get_llm
from app.schemas.interview import InterviewQuestion


# è¾…åŠ©æ¨¡å‹ï¼šç”¨äºè§£æåˆ—è¡¨
class QuestionList(BaseModel):
    questions: List[InterviewQuestion]


async def generate_tech_async(
        tech_stack: List[str],
        level: str,
        kb_context: str = "",
        chat_history: List[str] = None,
        user_profile: str = "",  # æ¥æ”¶å‚æ•°
) -> List[InterviewQuestion]:
    # 1. å¤„ç†é»˜è®¤å€¼
    if chat_history is None:
        chat_history = []

    # 2. æ‹¼æ¥å†å²è®°å½•å­—ç¬¦ä¸²
    history_str = "\n".join(chat_history[-5:]) if chat_history else "æ— å†å²å¯¹è¯"

    llm = get_llm(temperature=0.7)
    parser = PydanticOutputParser(pydantic_object=QuestionList)

    # 3. åŠ¨æ€æ„å»ºä¸Šä¸‹æ–‡æŒ‡ä»¤
    context_instruction = ""
    if kb_context:
        context_instruction = f"""
        ã€å‚è€ƒçŸ¥è¯†åº“ã€‘ï¼š
        ä»¥ä¸‹æ˜¯è¯¥ç”¨æˆ·ä¸ªäººåšå®¢ä¸­çš„ç›¸å…³æŠ€æœ¯ç¬”è®°ï¼Œè¯·ä¼˜å…ˆå‚è€ƒè¿™äº›å†…å®¹æ¥å‡ºé¢˜ï¼š
        {kb_context}
        """

    # 4. æ„å»º Prompt
    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æŠ€æœ¯é¢è¯•å®˜ã€‚

        ã€å½“å‰ä»»åŠ¡ã€‘ï¼š
        åŸºäºæŠ€æœ¯æ ˆ [{tech_stack}] å’ŒèŒçº§ [{level}] ç”Ÿæˆ 3 é“é¢è¯•é¢˜ã€‚

        {context_instruction}

        {user_profile}

        ã€å†å²å¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆMemoryï¼‰ã€‘ï¼š
        {history_str}
        (æ³¨æ„ï¼šå¦‚æœç”¨æˆ·åœ¨å†å²å¯¹è¯ä¸­æŒ‡å‡ºäº†åå¥½ï¼Œè¯·éµå¾ªï¼›å¦åˆ™è¯·å¿½ç•¥)

        ã€è¦æ±‚ã€‘ï¼š
        1. é¢˜ç›®è¦æœ‰æ·±åº¦ï¼Œè€ƒå¯Ÿåº•å±‚åŸç†æˆ–å®æˆ˜æ’é”™ã€‚
        2. æ¯é“é¢˜éƒ½è¦æä¾›ç®€ç»ƒçš„å‚è€ƒå›ç­”è¦ç‚¹ã€‚
        3. ç±»åˆ«æ ‡è®°ä¸º 'Technical'ã€‚

        è¯·ä¸¥æ ¼æŒ‰ç…§ JSON æ ¼å¼è¾“å‡º:
        {format_instructions}
        """
    )

    chain = prompt | llm | parser

    # 5. æ‰§è¡Œ (ğŸ”´ æ ¸å¿ƒä¿®å¤ï¼šå¿…é¡»æŠŠ user_profile ä¼ è¿›å»ï¼)
    result = await chain.ainvoke({
        "tech_stack": ", ".join(tech_stack),
        "level": level,
        "history_str": history_str,
        "user_profile": user_profile,  # <--- ä¹‹å‰æ¼äº†è¿™è¡Œï¼Œå¯¼è‡´ KeyError
        "context_instruction": context_instruction,
        "format_instructions": parser.get_format_instructions()
    })

    return result.questions