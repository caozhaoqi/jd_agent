from app.core.graph_state import AgentState
from app.core.llm_factory import get_llm
from app.chains.jd_parser import parse_jd_async
from app.chains.company_research import research_company  # è®°å¾—å¯¼å…¥è¿™ä¸ª
from app.chains.tech_gen import generate_tech_async
from app.chains.hr_gen import generate_hr_async
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field
from loguru import logger

# --- Node 1: JD Parser ---
async def jd_parser_node(state: AgentState):
    logger.debug("ğŸ” [Agent: Parser] æ­£åœ¨åˆ†æ JD...")
    meta = await parse_jd_async(state["jd_text"])
    return {
        "company_name": meta.company_name,
        "tech_stack": meta.tech_stack,
        "years_required": meta.years_required,
        "iteration_count": 0
    }


# --- Node 2: Researcher ---
async def researcher_node(state: AgentState):
    logger.debug("ğŸ•µï¸ [Agent: Researcher] æ­£åœ¨èƒŒè°ƒå…¬å¸...")
    info = await research_company(state["company_name"])
    return {"company_info": info}


# --- Node 3: Tech Lead ---
async def tech_lead_node(state: AgentState):
    iteration = state.get("iteration_count", 0)
    logger.debug(f"ğŸ’» [Agent: TechLead] å¼€å§‹å‡ºé¢˜ (ç¬¬ {iteration + 1} ç‰ˆ)...")

    # è·å–åé¦ˆ
    feedback = state.get("review_comment", "")
    human_msg = state.get("human_feedback", "")

    # è¿™é‡Œç®€å•å¤„ç†ï¼Œå®é™…åº”ä¿®æ”¹ generate_tech_async æ¥å— context
    # ä¸ºäº†è·‘é€šï¼Œæˆ‘ä»¬æš‚ä¸ä¼  contextï¼Œæˆ–è€…ä½ ä¿®æ”¹ generate_tech_async
    questions = await generate_tech_async(
        state["tech_stack"],
        state["years_required"]
    )

    return {
        "tech_questions": questions,
        "iteration_count": iteration + 1,
        "human_feedback": None
    }


# --- Node 4: HR Agent ---
async def hr_node(state: AgentState):
    logger.debug("ğŸ‘” [Agent: HR] æ­£åœ¨ç”Ÿæˆè¡Œä¸ºé¢è¯•é¢˜...")
    questions = await generate_hr_async(
        ["æ²Ÿé€šèƒ½åŠ›", "æŠ—å‹èƒ½åŠ›"],
        state.get("company_info", "")
    )
    return {"hr_questions": questions}


# --- Node 5: Reviewer ---
class ReviewResult(BaseModel):
    score: int = Field(description="0-100åˆ†")
    comment: str = Field(description="å…·ä½“çš„ä¿®æ”¹å»ºè®®ï¼Œå¦‚æœæ»¡åˆ†åˆ™ç•™ç©º")


async def reviewer_node(state: AgentState):
    logger.debug("âš–ï¸ [Agent: QA] æ­£åœ¨å®¡æ ¸é¢˜ç›®è´¨é‡...")
    llm = get_llm(temperature=0.1)
    parser = JsonOutputParser(pydantic_object=ReviewResult)

    prompt = ChatPromptTemplate.from_template(
        """
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æŠ€æœ¯é¢è¯•é¢˜è´¨æ£€å‘˜ã€‚
        å¾…å®¡æ ¸é¢˜ç›®ï¼š{questions}
        å€™é€‰äººèŒçº§ï¼š{level}
        è¯·è¯„åˆ† (0-100) å¹¶ç»™å‡ºä¿®æ”¹å»ºè®®ã€‚åªè¾“å‡º JSONã€‚
        {format_instructions}
        """
    )
    chain = prompt | llm | parser
    try:
        result = await chain.ainvoke({
            "questions": str(state["tech_questions"]),
            "level": state["years_required"],
            "format_instructions": parser.get_format_instructions()
        })
    except Exception:
        result = {"score": 95, "comment": "è§£æå¤±è´¥ï¼Œé»˜è®¤é€šè¿‡"}

    logger.debug(f"ğŸ“Š [QA Result] Score: {result['score']}")
    return {"quality_score": result['score'], "review_comment": result['comment']}


# --- Node 6: Human Approval (å ä½ç¬¦) ---
def human_approval_node(state: AgentState):
    logger.debug("ğŸ›‘ [System] ä»»åŠ¡æš‚åœï¼šç­‰å¾…äººå·¥å®¡æ ¸ (Human-in-the-loop)...")
    pass