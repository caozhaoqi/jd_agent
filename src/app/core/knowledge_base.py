import os
import torch
from typing import List, Dict, Any, Union, Coroutine
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from app.utils.logger import logger

# 1. ç¡®å®šå‘é‡åº“è·¯å¾„
# é€»è¾‘ï¼šå½“å‰æ–‡ä»¶ -> ä¸Šçº§(core) -> ä¸Šçº§(app) -> ä¸Šçº§(src) -> é¡¹ç›®æ ¹ç›®å½• -> blog_faiss_index
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(CURRENT_DIR)))
DB_PATH = os.path.join(PROJECT_ROOT, "blog_faiss_index")


class BlogKnowledgeBase:
    _instance = None

    def __new__(cls):
        """å•ä¾‹æ¨¡å¼ï¼šç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªçŸ¥è¯†åº“å®ä¾‹ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹å ç”¨å†…å­˜"""
        if cls._instance is None:
            cls._instance = super(BlogKnowledgeBase, cls).__new__(cls)
            cls._instance._initialize()
        return cls._instance

    def _initialize(self):
        """åˆå§‹åŒ–åŠ è½½æ¨¡å‹å’Œå‘é‡åº“"""
        logger.info("ğŸ“š [KB] Initializing Blog Knowledge Base...")
        try:
            # 2. è‡ªåŠ¨æ£€æµ‹æœ€ä½³ç¡¬ä»¶è®¾å¤‡ (MPS > CUDA > CPU)
            if torch.backends.mps.is_available():
                # é€‚é… macOS Mç³»åˆ—èŠ¯ç‰‡ (M1/M2/M3/M4)
                device = "mps"
                logger.info("ğŸš€ [KB] Using Apple Metal (MPS) acceleration!")
            elif torch.cuda.is_available():
                # é€‚é… NVIDIA æ˜¾å¡
                device = "cuda"
                logger.info("ğŸš€ [KB] Using CUDA acceleration!")
            else:
                # å…œåº• CPU
                device = "cpu"
                logger.info("ğŸ¢ [KB] No GPU detected. Using CPU.")

            # 3. åˆå§‹åŒ– Embedding æ¨¡å‹
            # è®¾ç½® HF é•œåƒï¼Œé˜²æ­¢å›½å†…ç½‘ç»œä¸‹è½½æ¨¡å‹è¶…æ—¶
            os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

            self.embeddings = HuggingFaceEmbeddings(
                model_name="BAAI/bge-small-zh-v1.5",
                # å…³é”®ä¿®æ”¹ï¼šå°† device è®¾ç½®ä¸ºæ£€æµ‹åˆ°çš„ç¡¬ä»¶ï¼Œè€Œä¸æ˜¯å†™æ­» 'cpu'
                model_kwargs={'device': device},
                encode_kwargs={'normalize_embeddings': True}
            )

            # 4. åŠ è½½ FAISS å‘é‡åº“
            if os.path.exists(DB_PATH):
                self.vector_store = FAISS.load_local(
                    DB_PATH,
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                logger.success(f"âœ… [KB] Vector Store loaded successfully from: {DB_PATH}")
            else:
                logger.warning(f"âš ï¸ [KB] Index not found at {DB_PATH}. RAG functionality disabled.")
                self.vector_store = None

        except Exception as e:
            logger.error(f"âŒ [KB] Init failed: {e}")
            self.vector_store = None

    async def search(self, query: str, top_k: int = 3) -> dict[str, Union[str, list[Any]]]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        è¿”å›æ ¼å¼: {"context": "æ‹¼æ¥å¥½çš„æ–‡æ¡£å†…å®¹...", "sources": ["æ–‡ç« A.md", "æ–‡ç« B.md"]}
        """
        if not self.vector_store:
            return {"context": "", "sources": []}

        try:
            # å¼‚æ­¥æ‰§è¡Œæœç´¢
            # æ³¨æ„ï¼šFAISS ç´¢å¼•æœç´¢æ˜¯åœ¨ CPU ä¸Šè¿›è¡Œçš„ï¼Œä½†åœ¨ Web æœåŠ¡ä¸­éå¸¸å¿«
            # Embedding çš„ç”Ÿæˆï¼ˆå°† query è½¬ä¸ºå‘é‡ï¼‰ä¼šä½¿ç”¨ä¸Šé¢é…ç½®çš„ device (MPS/GPU)
            docs = self.vector_store.similarity_search(query, k=top_k)

            if not docs:
                return {"context": "", "sources": []}

            # æ‹¼æ¥å†…å®¹
            context_parts = []
            sources = set()

            for doc in docs:
                # è·å–å…ƒæ•°æ®ä¸­çš„æ¥æºæ–‡ä»¶åï¼Œé»˜è®¤ä¸º"æœªçŸ¥æ¥æº"
                source = doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                sources.add(source)
                # æ ¼å¼åŒ–æ–‡æ¡£å†…å®¹
                context_parts.append(f"---[å¼•ç”¨è‡ª: {source}]---\n{doc.page_content}")

            return {
                "context": "\n\n".join(context_parts),
                "sources": list(sources)
            }
        except Exception as e:
            logger.error(f"âŒ [KB] Search failed: {e}")
            return {"context": "", "sources": []}


# å¯¼å‡ºå•ä¾‹å®ä¾‹
kb_engine = BlogKnowledgeBase()